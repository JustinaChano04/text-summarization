{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d77b723-9620-4772-8220-ce25f4cd355f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/justinchanchan8/miniconda3/envs/ts/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/justinchanchan8/miniconda3/envs/ts/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda117.so\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 7.5\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /home/justinchanchan8/miniconda3/envs/ts/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/justinchanchan8/miniconda3/envs/ts/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /home/justinchanchan8/miniconda3/envs/ts did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/home/justinchanchan8/miniconda3/envs/ts/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
      "  warn(msg)\n",
      "/home/justinchanchan8/miniconda3/envs/ts/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so'), PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
    "from peft import LoraConfig, get_peft_model, PeftConfig, PeftModel\n",
    "from peft import prepare_model_for_kbit_training\n",
    "from finetune_data_preprocessing import get_data\n",
    "from torch import cuda\n",
    "import torch\n",
    "import os\n",
    "ROOT = os.getcwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb1ca1dd-d47b-4211-b2b6-9c43e640899f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "cuda.empty_cache()\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a9337830-4778-4ed3-811b-ca575b41aea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd\n",
    "\n",
    "def gen_prompt(text_input):\n",
    "    return f\"\"\"\n",
    "    <human>: {text_input[\"Term\"]}\n",
    "    <assistant>: {text_input[\"Definition\"]}\n",
    "    \"\"\".strip()\n",
    "\n",
    "def gen_and_tok_prompt(text_input):\n",
    "    full_input = gen_prompt(text_input)\n",
    "    tok_full_prompt = tokenizer(full_input, padding = True , truncation =True)\n",
    "    return tok_full_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4916f65e-c62f-49d3-af38-bd8faf47be90",
   "metadata": {},
   "source": [
    "#### Load Finance Fine-tuning Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bc0fc53c-f8a6-4a88-a81c-7a0102acc5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_finance = get_data('/home/justinchanchan8/text-summarization/backend/data/finance_vocab.csv')\n",
    "data = Dataset.from_pandas(df_finance[['Term', 'Definition']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a23599ab-f4c5-488a-9ddf-92923828a909",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"fp4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888a0077-4a8e-4fee-b85f-f7c9920f84da",
   "metadata": {},
   "source": [
    "### Load pretrained Falcon 7B model from model/ directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c017ff9a-4239-4488-92ce-ef788f975684",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     \"tiiuae/falcon-7b-instruct\",\n",
    "# #     load_in_8bit=True,\n",
    "#     device_map='auto',\n",
    "#     trust_remote_code=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b63f3f81-ec3f-4f61-93b9-fe45ad5368d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|███████████████████████████| 2/2 [00:07<00:00,  3.67s/it]\n"
     ]
    }
   ],
   "source": [
    "model_4bit = AutoModelForCausalLM.from_pretrained(\n",
    "        \"tiiuae/falcon-7b-instruct\", \n",
    "        device_map=\"auto\",\n",
    "        quantization_config=quantization_config,\n",
    "        )\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"tiiuae/falcon-7b-instruct\")\n",
    "# tokenizer.return_token_type_ids = False\n",
    "\n",
    "# lora_config = LoraConfig(\n",
    "#     init_lora_weights=False\n",
    "# )\n",
    "# model_4bit.add_adapter(lora_config, adapter_name=\"adapter_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dcbff3c1-85a1-408f-956c-3b71da9ec006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_pretrained('/home/justinchanchan8/text-summarization/backend/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "83366deb-511c-4886-91bc-37624fc67aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    }
   ],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data = data.map(gen_and_tok_prompt)\n",
    "tokenizer.return_token_ids=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3861f523-fd9d-458d-b1a5-1f6a24087115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipe = pipeline(\n",
    "#         task=\"summarization\",\n",
    "#         model=model_4bit,\n",
    "#         tokenizer=tokenizer,\n",
    "#         use_cache=True,\n",
    "#         device_map=\"auto\",\n",
    "#         max_length=296,\n",
    "#         do_sample=True,\n",
    "#         top_k=10,\n",
    "#         num_return_sequences=1,\n",
    "#         eos_token_id=tokenizer.eos_token_id,\n",
    "#         pad_token_id=tokenizer.eos_token_id,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ebe8da3c-dde5-4fa0-b0e4-751fa6700588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipe(\"\"\"How are you?\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f546df08-63df-40e2-98f5-4bc9f71193f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "model_4bit.gradient_checkpointing_enable()\n",
    "model_4bit = prepare_model_for_kbit_training(model_4bit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3c2e0046-fc61-480e-938a-1e264cc0c408",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "858c9776-263d-498d-817a-f420942ab110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4718592 || all params: 3613463424 || trainable%: 0.13058363808693696\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"query_key_value\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model_4bit = get_peft_model(model_4bit, config)\n",
    "print_trainable_parameters(model_4bit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e7a0d494-cc0b-4321-aa24-4cb59f3bf728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = model.to(device)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9f82cbbd-488d-4b57-a1fa-ee288cb3592e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using 8-bit optimizers with a version of `bitsandbytes` < 0.41.1. It is recommended to update your version as a major bug has been fixed in 8-bit optimizers.\n",
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:04, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3, training_loss=2.5785682996114097, metrics={'train_runtime': 7.1886, 'train_samples_per_second': 18.78, 'train_steps_per_second': 0.417, 'total_flos': 198014033306880.0, 'train_loss': 2.5785682996114097, 'epoch': 2.0})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = transformers.TrainingArguments(\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    save_total_limit=4,\n",
    "    logging_steps=25,\n",
    "    output_dir=ROOT, # give the location where you want to store checkpoints \n",
    "    save_strategy='epoch',\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    lr_scheduler_type = 'cosine',\n",
    "    warmup_ratio = 0.05,\n",
    ")\n",
    "trainer = transformers.Trainer(\n",
    "    model=model_4bit,\n",
    "    train_dataset=data,\n",
    "    args=training_args,\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "model_4bit.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3a0ee852-2d60-453a-9dbd-b65066113b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = os.getcwd()\n",
    "model_4bit.save_pretrained(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8088400-6777-417a-85e7-e56cb63a1b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: You are currently loading Falcon using legacy code contained in the model repository. Falcon has now been fully ported into the Hugging Face transformers library. For the most up-to-date and high-performance version of the Falcon model code, please update to the latest version of transformers and then load the model without the trust_remote_code=True argument.\n",
      "\n",
      "Loading checkpoint shards: 100%|███████████████████████████| 2/2 [00:05<00:00,  2.84s/it]\n"
     ]
    }
   ],
   "source": [
    "root = os.getcwd()\n",
    "config = PeftConfig.from_pretrained(root)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "#     load_in_8bit=True,\n",
    "#     device_map='auto',\n",
    "    trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f83b434-3817-457f-be19-00f888e4cd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model_inf = PeftModel.from_pretrained(model,root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0922591-d26b-4f24-882f-00d9347d7f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<human>: Summarize this financial article. Afterwards, define and find the definition of terms about finance a middle schooler may not understand in the format of \"Term: Definition\". :\n",
      "    Chevron said it would buy Hess in an all-stock deal worth $53 billion, the second major oil tie-up this month following Exxon Mobil’s deal to buy Pioneer Natural Resources.\n",
      "    The U.S. energy company said buying Hess would upgrade and diversify its portfolio, marking Chevron’s entrance into an Exxon-led partnership overseeing a generational oil find in Guyana, while picking up additional U.S. shale assets largely in North Dakota. Chevron also highlighted the attraction of Hess’s assets in the Gulf of Mexico and its natural-gas business in Southeast Asia.\n",
      "    The transaction, less than two weeks after Exxon’s acquisition of Pioneer, shows America’s second-largest oil company is similarly tying its future to a bet that the world will continue to have an appetite for oil for decades, even as many countries are seeking to cut emissions and transition toward green energy.\n",
      "    The major oil companies’ doubling down on fossil fuels follows a recent prediction by the International Energy Agency that fossil fuel demand globally would reach its zenith this decade.\n",
      "    The deal values Hess at $171 a share based on Chevron’s closing price on Friday, with Hess shareholders receiving 1.025 shares of Chevron for each Hess share. The price represents a premium of 4.9% from Hess’s closing share price Friday and 10.3% when compared with the stock’s 20-day average price.\n",
      "    Hess shares rose 0.9% to $164.47 in morning trading, while Chevron shares fell 2.4% to $162.90. Including debt, the transaction has a total enterprise value of $60 billion.\n",
      "    The deal comes as oil-and-gas companies are flush with cash after energy prices rocketed in the wake of Russia’s invasion of Ukraine, with executives signaling they were ready to deploy their war chests at the deals table.\n",
      "    <assistant>: \"Chevron said it would buy Hess in an all-stock deal worth $53 billion, the second major oil tie-up this month following Exxon Mobil’s deal to buy Pioneer Natural Resources.\"\n",
      "    Definition: \"Exxon Mobil\" - \"An American multinational oil and gas company that is the largest producer of crude oil in the world.\"\n",
      "    Definition: \"Hess Corporation\" - \"An American oil and gas exploration and production company that is the largest independent oil and gas producer in the United States.\"\n",
      "    Definition: \"U.S. energy company\" - \"A U.S. company that produces or distributes energy, such as oil, natural gas, or electricity.\"\n",
      "    Definition: \"U.S. energy partnership\" - \"A U.S. partnership between two or more companies that produces or distributes energy, such as oil, natural gas, or electricity.\"\n",
      "    Definition: \"U.S\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "    <human>: Summarize this financial article. Afterwards, define and find the definition of terms about finance a middle schooler may not understand in the format of \"Term: Definition\". :\n",
    "    Chevron said it would buy Hess in an all-stock deal worth $53 billion, the second major oil tie-up this month following Exxon Mobil’s deal to buy Pioneer Natural Resources.\n",
    "    The U.S. energy company said buying Hess would upgrade and diversify its portfolio, marking Chevron’s entrance into an Exxon-led partnership overseeing a generational oil find in Guyana, while picking up additional U.S. shale assets largely in North Dakota. Chevron also highlighted the attraction of Hess’s assets in the Gulf of Mexico and its natural-gas business in Southeast Asia.\n",
    "    The transaction, less than two weeks after Exxon’s acquisition of Pioneer, shows America’s second-largest oil company is similarly tying its future to a bet that the world will continue to have an appetite for oil for decades, even as many countries are seeking to cut emissions and transition toward green energy.\n",
    "    The major oil companies’ doubling down on fossil fuels follows a recent prediction by the International Energy Agency that fossil fuel demand globally would reach its zenith this decade.\n",
    "    The deal values Hess at $171 a share based on Chevron’s closing price on Friday, with Hess shareholders receiving 1.025 shares of Chevron for each Hess share. The price represents a premium of 4.9% from Hess’s closing share price Friday and 10.3% when compared with the stock’s 20-day average price.\n",
    "    Hess shares rose 0.9% to $164.47 in morning trading, while Chevron shares fell 2.4% to $162.90. Including debt, the transaction has a total enterprise value of $60 billion.\n",
    "    The deal comes as oil-and-gas companies are flush with cash after energy prices rocketed in the wake of Russia’s invasion of Ukraine, with executives signaling they were ready to deploy their war chests at the deals table.\n",
    "    <assistant>: \n",
    "    \"\"\".strip()\n",
    "\n",
    "# encode the prompt \n",
    "encoding = tokenizer(prompt, return_tensors= \"pt\").to(model.device)\n",
    "\n",
    "# set teh generation configuration params \n",
    "gen_config = model_inf.generation_config\n",
    "gen_config.max_new_tokens = 200\n",
    "gen_config.temperature = 0.2\n",
    "gen_config.top_p = 0.7\n",
    "gen_config.num_return_sequences = 1\n",
    "gen_config.pad_token_id = tokenizer.eos_token_id\n",
    "gen_config.eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# do the inference \n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(input_ids = encoding.input_ids, attention_mask = encoding.attention_mask,generation_config = gen_config )\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens = True ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499a6a7b-7603-46a6-98ed-b60a13cb24f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
